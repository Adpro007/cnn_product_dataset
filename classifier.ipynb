{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monster/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"GoogleNews-vectors-negative300.bin\"\n",
    "category_index = {\"clothing\":0, \"camera\":1, \"home\":2}\n",
    "category_reverse_index = dict((y,x) for (x,y) in category_index.items())\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure there are no null values in the datasets\n",
      "Has null values:  False\n",
      "Has null values:  False\n",
      "Has null values:  False\n"
     ]
    }
   ],
   "source": [
    "clothing = pd.read_csv('dataset/clothing.tsv', sep='\\t')\n",
    "cameras = pd.read_csv('dataset/cameras.tsv', sep='\\t')\n",
    "home = pd.read_csv('dataset/home.tsv', sep='\\t')\n",
    "\n",
    "dataset = [clothing, cameras, home]\n",
    "\n",
    "print(\"Make sure there are no null values in the datasets\")\n",
    "for data in dataset:\n",
    "    print('Has null values: ', data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text= text.strip().lower().split()\n",
    "    text = filter(lambda word: word not in STOPWORDS, text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    data['title'] = data['title'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_texts = clothing['title'] + cameras['title'] + home['title']\n",
    "all_texts = all_texts.drop_duplicates(keep=False)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "clothing_sequences = tokenizer.texts_to_sequences(clothing['title'])\n",
    "electronics_sequences = tokenizer.texts_to_sequences(cameras['title'])\n",
    "home_appliances_sequences = tokenizer.texts_to_sequences(home['title'])\n",
    "\n",
    "clothing_data = pad_sequences(clothing_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "electronics_data = pad_sequences(electronics_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "home_appliances_data = pad_sequences(home_appliances_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\t\tid\n",
      "--------------------\n",
      "sports\t\t16\n",
      "action\t\t13\n",
      "spy\t\t7\n",
      "pen\t\t55\n",
      "camera\t\t2\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "test_string = \"sports action spy pen camera\"\n",
    "print(\"word\\t\\tid\")\n",
    "print(\"-\" * 20)\n",
    "for word in test_string.split():\n",
    "    print(\"%s\\t\\t%s\" % (word, word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to Vector [[16, 13, 2], [7, 55, 2]]\n",
      "Padded Vector [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 16 13  2]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  7 55  2]]\n"
     ]
    }
   ],
   "source": [
    "test_sequence = tokenizer.texts_to_sequences([\"sports action camera\", \"spy pen camera\"])\n",
    "padded_sequence = pad_sequences(test_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"Text to Vector\", test_sequence)\n",
    "print(\"Padded Vector\", padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing: \t\t [1. 0. 0.]\n",
      "camera: \t\t [0. 1. 0.]\n",
      "home: \t\t\t [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"clothing: \\t\\t\", to_categorical(category_index[\"clothing\"], 3))\n",
    "print(\"camera: \\t\\t\", to_categorical(category_index[\"camera\"], 3))\n",
    "print(\"home: \\t\\t\\t\", to_categorical(category_index[\"home\"], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clothing shape:  (392721, 30)\n",
      "electronics shape:  (1347, 30)\n",
      "home appliances shape:  (11425, 2)\n",
      "----------\n",
      "combined data shape:  (405493, 30)\n",
      "combined category/label shape:  (405493, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"clothing shape: \", clothing_data.shape)\n",
    "print(\"electronics shape: \", electronics_data.shape)\n",
    "print(\"home appliances shape: \", home.shape)\n",
    "\n",
    "data = np.vstack((clothing_data, electronics_data, home_appliances_data))\n",
    "category = pd.concat([clothing['category'], cameras['category'], home['category']]).values\n",
    "category = to_categorical(category)\n",
    "print(\"-\"*10)\n",
    "print(\"combined data shape: \", data.shape)\n",
    "print(\"combined category/label shape: \", category.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.3\n",
    "indices = np.arange(data.shape[0]) # get sequence of row index\n",
    "np.random.shuffle(indices) # shuffle the row indexes\n",
    "data = data[indices] # shuffle data/product-titles/x-axis\n",
    "category = category[indices] # shuffle labels/category/y-axis\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = category[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = category[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 1473\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0], # or len(word_index) + 1\n",
    "                            embedding_matrix.shape[1], # or EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(embedding_layer)\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv1D(300, 3, padding='valid',activation='relu',strides=2))\n",
    "# model.add(Conv1D(150, 3, padding='valid',activation='relu',strides=2))\n",
    "# model.add(Conv1D(75, 3, padding='valid',activation='relu',strides=2))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(150,activation='sigmoid'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "# score = model.evaluate(x_val, y_val, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 300)           817200    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 28, 250)           225250    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 753       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 1,105,953\n",
      "Trainable params: 288,753\n",
      "Non-trainable params: 817,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "\n",
    "model_1 = Sequential()\n",
    "model_1.add(embedding_layer)\n",
    "model_1.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n",
    "model_1.add(GlobalMaxPooling1D())\n",
    "model_1.add(Dense(250))\n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dense(3))\n",
    "model_1.add(Activation('sigmoid'))\n",
    "model_1.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 283846 samples, validate on 121647 samples\n",
      "Epoch 1/5\n",
      "283846/283846 [==============================] - 395s 1ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 2/5\n",
      "283846/283846 [==============================] - 387s 1ms/step - loss: 7.3636e-04 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 3/5\n",
      "283846/283846 [==============================] - 385s 1ms/step - loss: 7.1943e-04 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 4/5\n",
      "283846/283846 [==============================] - 384s 1ms/step - loss: 6.3839e-04 - acc: 0.9999 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 5/5\n",
      "283846/283846 [==============================] - 381s 1ms/step - loss: 5.6420e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9999\n",
      "Test loss: 0.0018763479748700844\n",
      "Test accuracy: 0.9998602513831003\n"
     ]
    }
   ],
   "source": [
    "model_1.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=128)\n",
    "score = model_1.evaluate(x_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "model_1.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_1\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_1 = load_model('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Predicted category:  camera\n",
      "----------\n",
      "Clothing Probability:  5.4753313e-22\n",
      "Camera Probability:  0.03814595\n",
      "home probability:  2.4052846e-19\n"
     ]
    }
   ],
   "source": [
    "example_product = \"Nikon Coolpix A10 Point and Shoot Camera (Black)\"\n",
    "example_product = preprocess(example_product)\n",
    "example_sequence = tokenizer.texts_to_sequences([example_product])\n",
    "example_padded_sequence = pad_sequences(example_sequence, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"-\"*10)\n",
    "print(\"Predicted category: \", category_reverse_index[model_1.predict_classes(example_padded_sequence, verbose=0)[0]])\n",
    "print(\"-\"*10)\n",
    "probabilities = model_1.predict(example_padded_sequence, verbose=0)\n",
    "probabilities = probabilities[0]\n",
    "print(\"Clothing Probability: \",probabilities[category_index[\"clothing\"]] )\n",
    "print(\"Camera Probability: \",probabilities[category_index[\"camera\"]] )\n",
    "print(\"home probability: \",probabilities[category_index[\"home\"]] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
